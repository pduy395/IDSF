{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xoLz-n7wcnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e6db87c-2238-41b3-c5ed-f9442b7841e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: torch==2.2.1 in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.1)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.1)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.1)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.1)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.1)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.1)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.1)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.1)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.1) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.1)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.2.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.1) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting seqeval==0.0.12\n",
            "  Downloading seqeval-0.0.12.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval==0.0.12) (1.25.2)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.10/dist-packages (from seqeval==0.0.12) (2.15.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7415 sha256=66efe46cc04a5d79e96e5341d9402c48e784715562a530bd95309960dc6e311b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/6c/fc/7076d687ba54f32c7be7eaaded97df359ef3c8fee08a2d4efc\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n",
            "Collecting pytorch-crf==0.7.2\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch==2.2.1\n",
        "!pip install seqeval==0.0.12\n",
        "!pip install pytorch-crf==0.7.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUU6leyGw1LK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cjcopKSCBUn",
        "outputId": "09d04b81-7979-45f4-c6d4-d4a9b69a0c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  atis.zip\n",
            "   creating: data/atis/\n",
            "   creating: data/atis/dev/\n",
            "  inflating: data/atis/dev/label     \n",
            "  inflating: data/atis/dev/seq.in    \n",
            "  inflating: data/atis/dev/seq.out   \n",
            "  inflating: data/atis/intent_label.txt  \n",
            "  inflating: data/atis/slot_label.txt  \n",
            "   creating: data/atis/test/\n",
            "  inflating: data/atis/test/label    \n",
            "  inflating: data/atis/test/seq.in   \n",
            "  inflating: data/atis/test/seq.out  \n",
            "   creating: data/atis/train/\n",
            "  inflating: data/atis/train/label   \n",
            "  inflating: data/atis/train/seq.in  \n",
            "  inflating: data/atis/train/seq.out  \n"
          ]
        }
      ],
      "source": [
        "#unzip dataset\n",
        "! mkdir data\n",
        "! unzip atis.zip -d data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMjssFr8Jo26"
      },
      "source": [
        "# get intent label, slot label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x93YBeWZyKt8"
      },
      "outputs": [],
      "source": [
        "def vocab_process(data_dir):\n",
        "    slot_label_vocab = 'slot_label.txt'\n",
        "    intent_label_vocab = 'intent_label.txt'\n",
        "\n",
        "    train_dir = os.path.join(data_dir, 'train')\n",
        "\n",
        "    # intent\n",
        "    # create file intent_label.txt and write label from dataset\n",
        "    with open(os.path.join(train_dir, 'label'), 'r', encoding='utf-8') as f_r, open(os.path.join(data_dir, intent_label_vocab), 'w',\n",
        "                                                                                    encoding='utf-8') as f_w:\n",
        "        intent_vocab = set()\n",
        "        for line in f_r:\n",
        "            line = line.strip()\n",
        "            intent_vocab.add(line)\n",
        "\n",
        "        additional_tokens = [\"UNK\"]\n",
        "        for token in additional_tokens:\n",
        "            f_w.write(token + '\\n')\n",
        "\n",
        "        intent_vocab = sorted(list(intent_vocab))\n",
        "        for intent in intent_vocab:\n",
        "            f_w.write(intent + '\\n')\n",
        "\n",
        "    # slot\n",
        "    # create file slot_label.txt and write label from dataset\n",
        "    with open(os.path.join(train_dir, 'seq.out'), 'r', encoding='utf-8') as f_r, open(os.path.join('', slot_label_vocab), 'w',\n",
        "                                                                                      encoding='utf-8') as f_w:\n",
        "        slot_vocab = set()\n",
        "        for line in f_r:\n",
        "            line = line.strip()\n",
        "            slots = line.split()\n",
        "            for slot in slots:\n",
        "                slot_vocab.add(slot)\n",
        "\n",
        "        slot_vocab = sorted(list(slot_vocab), key=lambda x: (x[2:], x[:2]))\n",
        "\n",
        "        # Write additional tokens\n",
        "        additional_tokens = [\"PAD\", \"UNK\"]\n",
        "        for token in additional_tokens:\n",
        "            f_w.write(token + '\\n')\n",
        "\n",
        "        for slot in slot_vocab:\n",
        "            f_w.write(slot + '\\n')\n",
        "\n",
        "\n",
        "vocab_process('data/atis')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS8lAh8PMEDe"
      },
      "source": [
        "# preprocess, getdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5Xg323gYqQu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"\n",
        "    A single training/test example for simple sequence classification.\n",
        "\n",
        "    Args:\n",
        "        guid: Unique id for the example.\n",
        "        words: list. The words of the sequence.\n",
        "        intent_label: (Optional) string. The intent label of the example.\n",
        "        slot_labels: (Optional) list. The slot labels of the example.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, guid, words, intent_label=None, slot_labels=None):\n",
        "        self.guid = guid\n",
        "        self.words = words\n",
        "        self.intent_label = intent_label\n",
        "        self.slot_labels = slot_labels\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, intent_label_id, slot_labels_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.intent_label_id = intent_label_id\n",
        "        self.slot_labels_ids = slot_labels_ids\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n",
        "\n",
        "\n",
        "class JointProcessor(object):\n",
        "    \"\"\"Processor for the JointBERT data set \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "        self.intent_labels = get_intent_labels(args)\n",
        "        self.slot_labels = get_slot_labels(args)\n",
        "\n",
        "        self.input_text_file = 'seq.in'\n",
        "        self.intent_label_file = 'label'\n",
        "        self.slot_labels_file = 'seq.out'\n",
        "\n",
        "    @classmethod\n",
        "    def _read_file(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = []\n",
        "            for line in f:\n",
        "                lines.append(line.strip())\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, texts, intents, slots, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for i, (text, intent, slot) in enumerate(zip(texts, intents, slots)):\n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            # 1. input_text\n",
        "            words = text.split()  # Some are spaced twice\n",
        "            # 2. intent\n",
        "            intent_label = self.intent_labels.index(intent) if intent in self.intent_labels else self.intent_labels.index(\"UNK\")\n",
        "            # 3. slot\n",
        "            slot_labels = []\n",
        "            for s in slot.split():\n",
        "                slot_labels.append(self.slot_labels.index(s) if s in self.slot_labels else self.slot_labels.index(\"UNK\"))\n",
        "\n",
        "            assert len(words) == len(slot_labels)\n",
        "            examples.append(InputExample(guid=guid, words=words, intent_label=intent_label, slot_labels=slot_labels))\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, dev, test\n",
        "        \"\"\"\n",
        "        data_path = os.path.join(self.args.data_dir, self.args.task, mode)\n",
        "        logger.info(\"LOOKING AT {}\".format(data_path))\n",
        "        return self._create_examples(texts=self._read_file(os.path.join(data_path, self.input_text_file)),\n",
        "                                     intents=self._read_file(os.path.join(data_path, self.intent_label_file)),\n",
        "                                     slots=self._read_file(os.path.join(data_path, self.slot_labels_file)),\n",
        "                                     set_type=mode)\n",
        "\n",
        "\n",
        "processors = {\n",
        "    \"atis\": JointProcessor,\n",
        "}\n",
        "\n",
        "\n",
        "def convert_examples_to_features(examples, max_seq_len, tokenizer,\n",
        "                                 pad_token_label_id=-100,\n",
        "                                 cls_token_segment_id=0,\n",
        "                                 pad_token_segment_id=0,\n",
        "                                 sequence_a_segment_id=0,\n",
        "                                 mask_padding_with_zero=True):\n",
        "    # Setting based on the current model type\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    unk_token = tokenizer.unk_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 5000 == 0:\n",
        "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
        "\n",
        "        # Tokenize word by word (for NER)\n",
        "        tokens = []\n",
        "        slot_labels_ids = []\n",
        "        for word, slot_label in zip(example.words, example.slot_labels):\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            if not word_tokens:\n",
        "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
        "            tokens.extend(word_tokens)\n",
        "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
        "            slot_labels_ids.extend([int(slot_label)] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        # Account for [CLS] and [SEP]\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_len - special_tokens_count)]\n",
        "            slot_labels_ids = slot_labels_ids[:(max_seq_len - special_tokens_count)]\n",
        "\n",
        "        # Add [SEP] token\n",
        "        tokens += [sep_token]\n",
        "        slot_labels_ids += [pad_token_label_id]\n",
        "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "        # Add [CLS] token\n",
        "        tokens = [cls_token] + tokens\n",
        "        slot_labels_ids = [pad_token_label_id] + slot_labels_ids\n",
        "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "        # tokens are attended to.\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = max_seq_len - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
        "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "        slot_labels_ids = slot_labels_ids + ([pad_token_label_id] * padding_length)\n",
        "\n",
        "        assert len(input_ids) == max_seq_len, \"Error with input length {} vs {}\".format(len(input_ids), max_seq_len)\n",
        "        assert len(attention_mask) == max_seq_len, \"Error with attention mask length {} vs {}\".format(len(attention_mask), max_seq_len)\n",
        "        assert len(token_type_ids) == max_seq_len, \"Error with token type length {} vs {}\".format(len(token_type_ids), max_seq_len)\n",
        "        assert len(slot_labels_ids) == max_seq_len, \"Error with slot labels length {} vs {}\".format(len(slot_labels_ids), max_seq_len)\n",
        "\n",
        "        intent_label_id = int(example.intent_label)\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % example.guid)\n",
        "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
        "            logger.info(\"intent_label: %s (id = %d)\" % (example.intent_label, intent_label_id))\n",
        "            logger.info(\"slot_labels: %s\" % \" \".join([str(x) for x in slot_labels_ids]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          token_type_ids=token_type_ids,\n",
        "                          intent_label_id=intent_label_id,\n",
        "                          slot_labels_ids=slot_labels_ids\n",
        "                          ))\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, mode):\n",
        "    processor = processors[args.task](args)\n",
        "\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        'cached_{}_{}_{}_{}'.format(\n",
        "            mode,\n",
        "            args.task,\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n",
        "            args.max_seq_len\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        # Load data features from dataset file\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        if mode == \"train\":\n",
        "            examples = processor.get_examples(\"train\")\n",
        "        elif mode == \"dev\":\n",
        "            examples = processor.get_examples(\"dev\")\n",
        "        elif mode == \"test\":\n",
        "            examples = processor.get_examples(\"test\")\n",
        "        else:\n",
        "            raise Exception(\"For mode, Only train, dev, test is available\")\n",
        "\n",
        "        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
        "        pad_token_label_id = args.ignore_index\n",
        "        features = convert_examples_to_features(examples, args.max_seq_len, tokenizer,\n",
        "                                                pad_token_label_id=pad_token_label_id)\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_intent_label_ids = torch.tensor([f.intent_label_id for f in features], dtype=torch.long)\n",
        "    all_slot_labels_ids = torch.tensor([f.slot_labels_ids for f in features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask,\n",
        "                            all_token_type_ids, all_intent_label_ids, all_slot_labels_ids)\n",
        "    return dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95nGYw49J2GD"
      },
      "source": [
        "# last layer in nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoUtF7EFytda"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class IntentClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_intent_labels, dropout_rate=0.):\n",
        "        super(IntentClassifier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.linear = nn.Linear(input_dim, num_intent_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        return self.linear(x)\n",
        "\n",
        "\n",
        "class SlotClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_slot_labels, dropout_rate=0.):\n",
        "        super(SlotClassifier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.linear = nn.Linear(input_dim, num_slot_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout(x)\n",
        "        return self.linear(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7laCueGeKGhj"
      },
      "source": [
        "# build model jointBert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfS1b1WLtb3p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.models.bert.modeling_bert import BertPreTrainedModel, BertModel, BertConfig\n",
        "from torchcrf import CRF\n",
        "\n",
        "\n",
        "\n",
        "class JointBERT(BertPreTrainedModel):\n",
        "    def __init__(self, config, args, intent_label_lst, slot_label_lst):\n",
        "        super(JointBERT, self).__init__(config)\n",
        "        self.args = args\n",
        "        self.num_intent_labels = len(intent_label_lst)\n",
        "        self.num_slot_labels = len(slot_label_lst)\n",
        "\n",
        "        self.bert = BertModel(config=config)  # Load pretrained bert\n",
        "\n",
        "        self.intent_classifier = IntentClassifier(config.hidden_size, self.num_intent_labels, args.dropout_rate)\n",
        "        self.slot_classifier = SlotClassifier(config.hidden_size, self.num_slot_labels, args.dropout_rate)\n",
        "\n",
        "        if args.use_crf:\n",
        "            self.crf = CRF(num_tags=self.num_slot_labels, batch_first=True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, intent_label_ids, slot_labels_ids):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
        "                            token_type_ids=token_type_ids)  # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        sequence_output = outputs[0]\n",
        "        pooled_output = outputs[1]  # [CLS]\n",
        "\n",
        "        intent_logits = self.intent_classifier(pooled_output)\n",
        "        slot_logits = self.slot_classifier(sequence_output)\n",
        "\n",
        "        total_loss = 0\n",
        "        # 1. Intent Softmax\n",
        "        if intent_label_ids is not None:\n",
        "            if self.num_intent_labels == 1:\n",
        "                intent_loss_fct = nn.MSELoss()\n",
        "                intent_loss = intent_loss_fct(intent_logits.view(-1), intent_label_ids.view(-1))\n",
        "            else:\n",
        "                intent_loss_fct = nn.CrossEntropyLoss()\n",
        "                intent_loss = intent_loss_fct(intent_logits.view(-1, self.num_intent_labels), intent_label_ids.view(-1))\n",
        "            total_loss += intent_loss\n",
        "\n",
        "        # 2. Slot Softmax\n",
        "        if slot_labels_ids is not None:\n",
        "            if self.args.use_crf:\n",
        "                slot_loss = self.crf(slot_logits, slot_labels_ids, mask=attention_mask.byte(), reduction='mean')\n",
        "                slot_loss = -1 * slot_loss  # negative log-likelihood\n",
        "            else:\n",
        "                slot_loss_fct = nn.CrossEntropyLoss(ignore_index=self.args.ignore_index)\n",
        "                # Only keep active parts of the loss\n",
        "                if attention_mask is not None:\n",
        "                    active_loss = attention_mask.view(-1) == 1\n",
        "                    active_logits = slot_logits.view(-1, self.num_slot_labels)[active_loss]\n",
        "                    active_labels = slot_labels_ids.view(-1)[active_loss]\n",
        "                    slot_loss = slot_loss_fct(active_logits, active_labels)\n",
        "                else:\n",
        "                    slot_loss = slot_loss_fct(slot_logits.view(-1, self.num_slot_labels), slot_labels_ids.view(-1))\n",
        "            total_loss += self.args.slot_loss_coef * slot_loss\n",
        "\n",
        "        outputs = ((intent_logits, slot_logits),) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        outputs = (total_loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions) # Logits is a tuple of intent and slot logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lifbfQ2VakC"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpzheXmbWoPW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "from transformers import BertConfig\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, JointBERT, BertTokenizer)\n",
        "}\n",
        "\n",
        "#đường dẫn tới model\n",
        "MODEL_PATH_MAP = {\n",
        "    'bert': 'bert-base-uncased'\n",
        "}\n",
        "\n",
        "# trả về list intent label\n",
        "def get_intent_labels(args):\n",
        "    return [label.strip() for label in open(os.path.join(args.data_dir, args.task, args.intent_label_file), 'r', encoding='utf-8')]\n",
        "\n",
        "# trả về list slot label\n",
        "def get_slot_labels(args):\n",
        "    return [label.strip() for label in open(os.path.join(args.data_dir, args.task, args.slot_label_file), 'r', encoding='utf-8')]\n",
        "\n",
        "# trả về tokenizer của model\n",
        "def load_tokenizer(args):\n",
        "    return MODEL_CLASSES[args.model_type][2].from_pretrained(args.model_name_or_path)\n",
        "\n",
        "\n",
        "def init_logger():\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO)\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if not args.no_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "#tính % đúng intent\n",
        "def get_intent_acc(preds, labels):\n",
        "    acc = (preds == labels).mean()\n",
        "    labels = [[str(label) for label in labels]]\n",
        "    preds = [[str(pred) for pred in preds]]\n",
        "    return {\n",
        "        \"intent_acc\": acc,\n",
        "        \"intent_precision\": precision_score(labels, preds),\n",
        "        \"intent_recall\": recall_score(labels, preds),\n",
        "        \"intent_f1\": f1_score(labels, preds)\n",
        "    }\n",
        "# tính điểm slot\n",
        "def get_slot_metrics(preds, labels):\n",
        "    assert len(preds) == len(labels)\n",
        "    return {\n",
        "        \"slot_precision\": precision_score(labels, preds),\n",
        "        \"slot_recall\": recall_score(labels, preds),\n",
        "        \"slot_f1\": f1_score(labels, preds)\n",
        "    }\n",
        "\n",
        "def collect_incorrect_samples(intent_preds, intent_labels, slot_preds, slot_labels):\n",
        "    incorrect_samples = []\n",
        "    for i in range(len(intent_preds)):\n",
        "        if intent_preds[i] != intent_labels[i] or slot_preds[i] != slot_labels[i]:\n",
        "            incorrect_samples.append(('Sample {}'.format(i+1), intent_preds[i], intent_labels[i], slot_preds[i], slot_labels[i]))\n",
        "    return incorrect_samples\n",
        "\n",
        "\n",
        "def get_sentence_frame_acc(intent_preds, intent_labels, slot_preds, slot_labels):\n",
        "    \"\"\"For the cases that intent and all the slots are correct (in one sentence)\"\"\"\n",
        "    # Get the intent comparison result\n",
        "    intent_result = (intent_preds == intent_labels)\n",
        "\n",
        "    # Get the slot comparision result\n",
        "    slot_result = []\n",
        "    for preds, labels in zip(slot_preds, slot_labels):\n",
        "        assert len(preds) == len(labels)\n",
        "        one_sent_result = True\n",
        "        for p, l in zip(preds, labels):\n",
        "            if p != l:\n",
        "                one_sent_result = False\n",
        "                break\n",
        "        slot_result.append(one_sent_result)\n",
        "    slot_result = np.array(slot_result)\n",
        "\n",
        "    sementic_acc = np.multiply(intent_result, slot_result).mean()\n",
        "    return {\n",
        "        \"sementic_frame_acc\": sementic_acc\n",
        "    }\n",
        "\n",
        "def compute_metrics(intent_preds, intent_labels, slot_preds, slot_labels):\n",
        "    assert len(intent_preds) == len(intent_labels) == len(slot_preds) == len(slot_labels)\n",
        "    results = {}\n",
        "    intent_result = get_intent_acc(intent_preds, intent_labels)\n",
        "    slot_result = get_slot_metrics(slot_preds, slot_labels)\n",
        "    sementic_result = get_sentence_frame_acc(intent_preds, intent_labels, slot_preds, slot_labels)\n",
        "\n",
        "    results.update(intent_result)\n",
        "    results.update(slot_result)\n",
        "    results.update(sementic_result)\n",
        "\n",
        "    incorrect_samples = collect_incorrect_samples(intent_preds, intent_labels, slot_preds, slot_labels)\n",
        "\n",
        "\n",
        "    # Tạo DataFrame từ các mẫu sai\n",
        "    df = pd.DataFrame(incorrect_samples, columns=['Sample', 'Predicted Intent', 'True Intent', 'Predicted Slots', 'True Slots'])\n",
        "\n",
        "    # Xuất DataFrame vào file Excel\n",
        "    df.to_excel('incorrect_samples.xlsx', index=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def read_prediction_text(args):\n",
        "    return [text.strip() for text in open(os.path.join(args.pred_dir, args.pred_input_file), 'r', encoding='utf-8')]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP_ZMrJtNpnT"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_fGgi3xXwdu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class Trainer(object):\n",
        "    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n",
        "        self.args = args\n",
        "        self.train_dataset = train_dataset\n",
        "        self.dev_dataset = dev_dataset\n",
        "        self.test_dataset = test_dataset\n",
        "\n",
        "        self.intent_label_lst = get_intent_labels(args)\n",
        "        self.slot_label_lst = get_slot_labels(args)\n",
        "\n",
        "        # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n",
        "        self.pad_token_label_id = args.ignore_index\n",
        "\n",
        "        self.config_class, self.model_class, _ = MODEL_CLASSES[args.model_type]\n",
        "        self.config = self.config_class.from_pretrained(args.model_name_or_path, finetuning_task=args.task)\n",
        "        self.model = self.model_class.from_pretrained(args.model_name_or_path,\n",
        "                                                      config=self.config,\n",
        "                                                      args=args,\n",
        "                                                      intent_label_lst=self.intent_label_lst,\n",
        "                                                      slot_label_lst=self.slot_label_lst)\n",
        "\n",
        "        # GPU or CPU\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def train(self):\n",
        "        train_sampler = RandomSampler(self.train_dataset)\n",
        "        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler, batch_size=self.args.train_batch_size)\n",
        "\n",
        "        if self.args.max_steps > 0:\n",
        "            t_total = self.args.max_steps\n",
        "            self.args.num_train_epochs = self.args.max_steps // (len(train_dataloader) // self.args.gradient_accumulation_steps) + 1\n",
        "        else:\n",
        "            t_total = len(train_dataloader) // self.args.gradient_accumulation_steps * self.args.num_train_epochs\n",
        "\n",
        "        # Prepare optimizer and schedule (linear warmup and decay)\n",
        "        no_decay = ['bias', 'LayerNorm.weight']\n",
        "        optimizer_grouped_parameters = [\n",
        "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "             'weight_decay': self.args.weight_decay},\n",
        "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "        optimizer = torch.optim.Adam(optimizer_grouped_parameters, lr=self.args.learning_rate, eps=self.args.adam_epsilon)\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=self.args.warmup_steps, num_training_steps=t_total)\n",
        "\n",
        "        # Train!\n",
        "        logger.info(\"***** Running training *****\")\n",
        "        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n",
        "        logger.info(\"  Num Epochs = %d\", self.args.num_train_epochs)\n",
        "        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n",
        "        logger.info(\"  Gradient Accumulation steps = %d\", self.args.gradient_accumulation_steps)\n",
        "        logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "        logger.info(\"  Logging steps = %d\", self.args.logging_steps)\n",
        "        logger.info(\"  Save steps = %d\", self.args.save_steps)\n",
        "\n",
        "        global_step = 0\n",
        "        tr_loss = 0.0\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        train_iterator = trange(int(self.args.num_train_epochs), desc=\"Epoch\")\n",
        "\n",
        "        for _ in train_iterator:\n",
        "            epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
        "            for step, batch in enumerate(epoch_iterator):\n",
        "                self.model.train()\n",
        "                batch = tuple(t.to(self.device) for t in batch)  # GPU or CPU\n",
        "\n",
        "                inputs = {'input_ids': batch[0],\n",
        "                          'attention_mask': batch[1],\n",
        "                          'intent_label_ids': batch[3],\n",
        "                          'slot_labels_ids': batch[4]}\n",
        "                if self.args.model_type != 'distilbert':\n",
        "                    inputs['token_type_ids'] = batch[2]\n",
        "                outputs = self.model(**inputs)\n",
        "                loss = outputs[0]\n",
        "\n",
        "                if self.args.gradient_accumulation_steps > 1:\n",
        "                    loss = loss / self.args.gradient_accumulation_steps\n",
        "\n",
        "                loss.backward()\n",
        "\n",
        "                tr_loss += loss.item()\n",
        "                if (step + 1) % self.args.gradient_accumulation_steps == 0:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n",
        "\n",
        "                    optimizer.step()\n",
        "                    scheduler.step()  # Update learning rate schedule\n",
        "                    self.model.zero_grad()\n",
        "                    global_step += 1\n",
        "\n",
        "                    if self.args.logging_steps > 0 and global_step % self.args.logging_steps == 0:\n",
        "                        self.evaluate(\"dev\")\n",
        "\n",
        "                    if self.args.save_steps > 0 and global_step % self.args.save_steps == 0:\n",
        "                        self.save_model()\n",
        "\n",
        "                if 0 < self.args.max_steps < global_step:\n",
        "                    epoch_iterator.close()\n",
        "                    break\n",
        "\n",
        "            if 0 < self.args.max_steps < global_step:\n",
        "                train_iterator.close()\n",
        "                break\n",
        "\n",
        "        return global_step, tr_loss / global_step\n",
        "\n",
        "    def evaluate(self, mode):\n",
        "        if mode == 'test':\n",
        "            dataset = self.test_dataset\n",
        "        elif mode == 'dev':\n",
        "            dataset = self.dev_dataset\n",
        "        else:\n",
        "            raise Exception(\"Only dev and test dataset available\")\n",
        "\n",
        "        eval_sampler = SequentialSampler(dataset)\n",
        "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=self.args.eval_batch_size)\n",
        "\n",
        "        # Eval!\n",
        "        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n",
        "        logger.info(\"  Num examples = %d\", len(dataset))\n",
        "        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n",
        "        eval_loss = 0.0\n",
        "        nb_eval_steps = 0\n",
        "        intent_preds = None\n",
        "        slot_preds = None\n",
        "        out_intent_label_ids = None\n",
        "        out_slot_labels_ids = None\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
        "            batch = tuple(t.to(self.device) for t in batch)\n",
        "            with torch.no_grad():\n",
        "                inputs = {'input_ids': batch[0],\n",
        "                          'attention_mask': batch[1],\n",
        "                          'intent_label_ids': batch[3],\n",
        "                          'slot_labels_ids': batch[4]}\n",
        "                if self.args.model_type != 'distilbert':\n",
        "                    inputs['token_type_ids'] = batch[2]\n",
        "                outputs = self.model(**inputs)\n",
        "                tmp_eval_loss, (intent_logits, slot_logits) = outputs[:2]\n",
        "\n",
        "                eval_loss += tmp_eval_loss.mean().item()\n",
        "            nb_eval_steps += 1\n",
        "\n",
        "            # Intent prediction\n",
        "            if intent_preds is None:\n",
        "                intent_preds = intent_logits.detach().cpu().numpy()\n",
        "                out_intent_label_ids = inputs['intent_label_ids'].detach().cpu().numpy()\n",
        "            else:\n",
        "                intent_preds = np.append(intent_preds, intent_logits.detach().cpu().numpy(), axis=0)\n",
        "                out_intent_label_ids = np.append(\n",
        "                    out_intent_label_ids, inputs['intent_label_ids'].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "            # Slot prediction\n",
        "            if slot_preds is None:\n",
        "                if self.args.use_crf:\n",
        "                    # decode() in `torchcrf` returns list with best index directly\n",
        "                    slot_preds = np.array(self.model.crf.decode(slot_logits))\n",
        "                else:\n",
        "                    slot_preds = slot_logits.detach().cpu().numpy()\n",
        "\n",
        "                out_slot_labels_ids = inputs[\"slot_labels_ids\"].detach().cpu().numpy()\n",
        "            else:\n",
        "                if self.args.use_crf:\n",
        "                    slot_preds = np.append(slot_preds, np.array(self.model.crf.decode(slot_logits)), axis=0)\n",
        "                else:\n",
        "                    slot_preds = np.append(slot_preds, slot_logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "                out_slot_labels_ids = np.append(out_slot_labels_ids, inputs[\"slot_labels_ids\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "        eval_loss = eval_loss / nb_eval_steps\n",
        "        results = {\n",
        "            \"loss\": eval_loss\n",
        "        }\n",
        "\n",
        "        # Intent result\n",
        "        intent_preds = np.argmax(intent_preds, axis=1)\n",
        "\n",
        "        # Slot result\n",
        "        if not self.args.use_crf:\n",
        "            slot_preds = np.argmax(slot_preds, axis=2)\n",
        "        slot_label_map = {i: label for i, label in enumerate(self.slot_label_lst)}\n",
        "        out_slot_label_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
        "        slot_preds_list = [[] for _ in range(out_slot_labels_ids.shape[0])]\n",
        "\n",
        "        for i in range(out_slot_labels_ids.shape[0]):\n",
        "            for j in range(out_slot_labels_ids.shape[1]):\n",
        "                if out_slot_labels_ids[i, j] != self.pad_token_label_id:\n",
        "                    out_slot_label_list[i].append(slot_label_map[out_slot_labels_ids[i][j]])\n",
        "                    slot_preds_list[i].append(slot_label_map[slot_preds[i][j]])\n",
        "\n",
        "\n",
        "        total_result = compute_metrics(intent_preds, out_intent_label_ids, slot_preds_list, out_slot_label_list)\n",
        "\n",
        "        results.update(total_result)\n",
        "\n",
        "        logger.info(\"***** Eval results *****\")\n",
        "        for key in sorted(results.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(results[key]))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_model(self):\n",
        "        # Save model checkpoint (Overwrite)\n",
        "        if not os.path.exists(self.args.model_dir):\n",
        "            os.makedirs(self.args.model_dir)\n",
        "        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n",
        "        model_to_save.save_pretrained(self.args.model_dir)\n",
        "\n",
        "        # Save training arguments together with the trained model\n",
        "        torch.save(self.args, os.path.join(self.args.model_dir, 'training_args.bin'))\n",
        "        logger.info(\"Saving model checkpoint to %s\", self.args.model_dir)\n",
        "\n",
        "    def load_model(self):\n",
        "        # Check whether model exists\n",
        "        if not os.path.exists(self.args.model_dir):\n",
        "            raise Exception(\"Model doesn't exists! Train first!\")\n",
        "\n",
        "        try:\n",
        "            self.model = self.model_class.from_pretrained(self.args.model_dir,\n",
        "                                                          args=self.args,\n",
        "                                                          intent_label_lst=self.intent_label_lst,\n",
        "                                                          slot_label_lst=self.slot_label_lst)\n",
        "            self.model.to(self.device)\n",
        "            logger.info(\"***** Model Loaded *****\")\n",
        "        except:\n",
        "            raise Exception(\"Some model files might be missing...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdIBTjUewlk0"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ1QMf6dX7cf",
        "outputId": "09de7840-0d4b-469c-eb4a-117365ce8cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:__main__:***** Running evaluation on dev dataset *****\n",
            "INFO:__main__:  Num examples = 500\n",
            "INFO:__main__:  Batch size = 64\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▎        | 1/8 [00:00<00:01,  5.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  25%|██▌       | 2/8 [00:00<00:01,  5.19it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  38%|███▊      | 3/8 [00:00<00:00,  5.31it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  50%|█████     | 4/8 [00:00<00:00,  5.39it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▎   | 5/8 [00:00<00:00,  5.40it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 6/8 [00:01<00:00,  5.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 7/8 [00:01<00:00,  5.47it/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 8/8 [00:01<00:00,  5.43it/s]\n",
            "INFO:__main__:***** Eval results *****\n",
            "INFO:__main__:  intent_acc = 0.978\n",
            "INFO:__main__:  intent_f1 = 0.9263157894736841\n",
            "INFO:__main__:  intent_precision = 0.9090909090909091\n",
            "INFO:__main__:  intent_recall = 0.944206008583691\n",
            "INFO:__main__:  loss = 0.1995230067986995\n",
            "INFO:__main__:  sementic_frame_acc = 0.918\n",
            "INFO:__main__:  slot_f1 = 0.9775182481751825\n",
            "INFO:__main__:  slot_precision = 0.9760932944606414\n",
            "INFO:__main__:  slot_recall = 0.9789473684210527\n",
            "INFO:__main__:Saving model checkpoint to atis_model\n",
            "\n",
            "Iteration: 100%|██████████| 140/140 [00:45<00:00,  3.05it/s]\n",
            "Epoch: 100%|██████████| 10/10 [07:05<00:00, 42.60s/it]\n",
            "INFO:__main__:***** Model Loaded *****\n",
            "INFO:__main__:***** Running evaluation on test dataset *****\n",
            "INFO:__main__:  Num examples = 893\n",
            "INFO:__main__:  Batch size = 64\n",
            "Evaluating: 100%|██████████| 14/14 [00:02<00:00,  5.52it/s]\n",
            "INFO:__main__:***** Eval results *****\n",
            "INFO:__main__:  intent_acc = 0.9798432250839866\n",
            "INFO:__main__:  intent_f1 = 0.9369369369369369\n",
            "INFO:__main__:  intent_precision = 0.9386281588447654\n",
            "INFO:__main__:  intent_recall = 0.935251798561151\n",
            "INFO:__main__:  loss = 0.2896911504545382\n",
            "INFO:__main__:  sementic_frame_acc = 0.8723404255319149\n",
            "INFO:__main__:  slot_f1 = 0.9527489899877043\n",
            "INFO:__main__:  slot_precision = 0.9502452697967765\n",
            "INFO:__main__:  slot_recall = 0.9552659387108137\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "def main(args):\n",
        "    init_logger()\n",
        "    set_seed(args)\n",
        "    tokenizer = load_tokenizer(args).from_pretrained('bert-base-uncased')\n",
        "\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\")\n",
        "    dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\")\n",
        "    test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\")\n",
        "\n",
        "    trainer = Trainer(args, train_dataset, dev_dataset, test_dataset)\n",
        "\n",
        "    if args.do_train:\n",
        "        trainer.train()\n",
        "\n",
        "    if args.do_eval:\n",
        "        trainer.load_model()\n",
        "        trainer.evaluate(\"test\")\n",
        "\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--task\", default=\"atis\",  type=str, help=\"The name of the task to train\")\n",
        "parser.add_argument(\"--model_dir\", default=\"atis_model\",  type=str, help=\"Path to save, load model\")\n",
        "parser.add_argument(\"--data_dir\", default=\"data\", type=str, help=\"The input data dir\")\n",
        "parser.add_argument(\"--intent_label_file\", default=\"intent_label.txt\", type=str, help=\"Intent Label file\")\n",
        "parser.add_argument(\"--slot_label_file\", default=\"slot_label.txt\", type=str, help=\"Slot Label file\")\n",
        "\n",
        "parser.add_argument(\"--model_type\", default=\"bert\", type=str, help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
        "\n",
        "parser.add_argument('--seed', type=int, default=1234, help=\"random seed for initialization\")\n",
        "parser.add_argument(\"--train_batch_size\", default=32, type=int, help=\"Batch size for training.\")\n",
        "parser.add_argument(\"--eval_batch_size\", default=64, type=int, help=\"Batch size for evaluation.\")\n",
        "parser.add_argument(\"--max_seq_len\", default=50, type=int, help=\"The maximum total input sequence length after tokenization.\")\n",
        "parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"The initial learning rate for Adam.\")\n",
        "parser.add_argument(\"--num_train_epochs\", default=10.0, type=float, help=\"Total number of training epochs to perform.\")\n",
        "parser.add_argument(\"--weight_decay\", default=0.0, type=float, help=\"Weight decay if we apply some.\")\n",
        "parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "                    help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "parser.add_argument(\"--adam_epsilon\", default=1e-8, type=float, help=\"Epsilon for Adam optimizer.\")\n",
        "parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
        "parser.add_argument(\"--max_steps\", default=-1, type=int, help=\"If > 0: set total number of training steps to perform. Override num_train_epochs.\")\n",
        "parser.add_argument(\"--warmup_steps\", default=0, type=int, help=\"Linear warmup over warmup_steps.\")\n",
        "parser.add_argument(\"--dropout_rate\", default=0.1, type=float, help=\"Dropout for fully-connected layers\")\n",
        "\n",
        "parser.add_argument('--logging_steps', type=int, default=200, help=\"Log every X updates steps.\")\n",
        "parser.add_argument('--save_steps', type=int, default=200, help=\"Save checkpoint every X updates steps.\")\n",
        "\n",
        "parser.add_argument(\"--do_train\",default=True, action=\"store_true\", help=\"Whether to run training.\")\n",
        "parser.add_argument(\"--do_eval\",default=True, action=\"store_true\", help=\"Whether to run eval on the test set.\")\n",
        "parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "\n",
        "parser.add_argument(\"--ignore_index\", default=0, type=int,\n",
        "                    help='Specifies a target value that is ignored and does not contribute to the input gradient')\n",
        "\n",
        "parser.add_argument('--slot_loss_coef', type=float, default=1.0, help='Coefficient for the slot loss.')\n",
        "\n",
        "# CRF option\n",
        "parser.add_argument(\"--use_crf\", action=\"store_true\", help=\"Whether to use CRF\")\n",
        "parser.add_argument(\"--slot_pad_label\", default=\"PAD\", type=str, help=\"Pad token for slot label pad (to be ignore when calculate loss)\")\n",
        "\n",
        "args = parser.parse_args([])\n",
        "\n",
        "args.model_name_or_path = MODEL_PATH_MAP[args.model_type]  #gắn đường dẫn đến file model\n",
        "main(args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkKe_okVum7U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import argparse\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def get_device(pred_config):\n",
        "    return \"cuda\" if torch.cuda.is_available() and not pred_config.no_cuda else \"cpu\"\n",
        "\n",
        "\n",
        "def get_args(pred_config):\n",
        "    return torch.load(os.path.join(pred_config.model_dir, 'training_args.bin'))\n",
        "\n",
        "\n",
        "def load_model(pred_config, args, device):\n",
        "    # Check whether model exists\n",
        "    if not os.path.exists(pred_config.model_dir):\n",
        "        raise Exception(\"Model doesn't exists! Train first!\")\n",
        "\n",
        "    try:\n",
        "        model = MODEL_CLASSES[args.model_type][1].from_pretrained(args.model_dir,\n",
        "                                                                  args=args,\n",
        "                                                                  intent_label_lst=get_intent_labels(args),\n",
        "                                                                  slot_label_lst=get_slot_labels(args))\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "        logger.info(\"***** Model Loaded *****\")\n",
        "    except:\n",
        "        raise Exception(\"Some model files might be missing...\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def read_input_file(pred_config):\n",
        "    lines = []\n",
        "    with open(pred_config.input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            words = line.split()\n",
        "            lines.append(words)\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "def convert_input_file_to_tensor_dataset(lines,\n",
        "                                         pred_config,\n",
        "                                         args,\n",
        "                                         tokenizer,\n",
        "                                         pad_token_label_id,\n",
        "                                         cls_token_segment_id=0,\n",
        "                                         pad_token_segment_id=0,\n",
        "                                         sequence_a_segment_id=0,\n",
        "                                         mask_padding_with_zero=True):\n",
        "    # Setting based on the current model type\n",
        "    cls_token = tokenizer.cls_token\n",
        "    sep_token = tokenizer.sep_token\n",
        "    unk_token = tokenizer.unk_token\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    all_input_ids = []\n",
        "    all_attention_mask = []\n",
        "    all_token_type_ids = []\n",
        "    all_slot_label_mask = []\n",
        "\n",
        "    for words in lines:\n",
        "        tokens = []\n",
        "        slot_label_mask = []\n",
        "        for word in words:\n",
        "            word_tokens = tokenizer.tokenize(word)\n",
        "            if not word_tokens:\n",
        "                word_tokens = [unk_token]  # For handling the bad-encoded word\n",
        "            tokens.extend(word_tokens)\n",
        "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
        "            slot_label_mask.extend([pad_token_label_id + 1] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        # Account for [CLS] and [SEP]\n",
        "        special_tokens_count = 2\n",
        "        if len(tokens) > args.max_seq_len - special_tokens_count:\n",
        "            tokens = tokens[: (args.max_seq_len - special_tokens_count)]\n",
        "            slot_label_mask = slot_label_mask[:(args.max_seq_len - special_tokens_count)]\n",
        "\n",
        "        # Add [SEP] token\n",
        "        tokens += [sep_token]\n",
        "        token_type_ids = [sequence_a_segment_id] * len(tokens)\n",
        "        slot_label_mask += [pad_token_label_id]\n",
        "\n",
        "        # Add [CLS] token\n",
        "        tokens = [cls_token] + tokens\n",
        "        token_type_ids = [cls_token_segment_id] + token_type_ids\n",
        "        slot_label_mask = [pad_token_label_id] + slot_label_mask\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
        "        attention_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "        # Zero-pad up to the sequence length.\n",
        "        padding_length = args.max_seq_len - len(input_ids)\n",
        "        input_ids = input_ids + ([pad_token_id] * padding_length)\n",
        "        attention_mask = attention_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([pad_token_segment_id] * padding_length)\n",
        "        slot_label_mask = slot_label_mask + ([pad_token_label_id] * padding_length)\n",
        "\n",
        "        all_input_ids.append(input_ids)\n",
        "        all_attention_mask.append(attention_mask)\n",
        "        all_token_type_ids.append(token_type_ids)\n",
        "        all_slot_label_mask.append(slot_label_mask)\n",
        "\n",
        "    # Change to Tensor\n",
        "    all_input_ids = torch.tensor(all_input_ids, dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor(all_attention_mask, dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor(all_token_type_ids, dtype=torch.long)\n",
        "    all_slot_label_mask = torch.tensor(all_slot_label_mask, dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_slot_label_mask)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def predict(pred_config):\n",
        "    # load model and args\n",
        "    args = get_args(pred_config)\n",
        "    device = get_device(pred_config)\n",
        "    model = load_model(pred_config, args, device)\n",
        "    logger.info(args)\n",
        "\n",
        "    intent_label_lst = get_intent_labels(args)\n",
        "    slot_label_lst = get_slot_labels(args)\n",
        "\n",
        "    # Convert input file to TensorDataset\n",
        "    pad_token_label_id = args.ignore_index\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    lines = read_input_file(pred_config)\n",
        "    dataset = convert_input_file_to_tensor_dataset(lines, pred_config, args, tokenizer, pad_token_label_id)\n",
        "\n",
        "    # Predict\n",
        "    sampler = SequentialSampler(dataset)\n",
        "    data_loader = DataLoader(dataset, sampler=sampler, batch_size=pred_config.batch_size)\n",
        "\n",
        "    all_slot_label_mask = None\n",
        "    intent_preds = None\n",
        "    slot_preds = None\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        with torch.no_grad():\n",
        "            inputs = {\"input_ids\": batch[0],\n",
        "                      \"attention_mask\": batch[1],\n",
        "                      \"intent_label_ids\": None,\n",
        "                      \"slot_labels_ids\": None}\n",
        "            if args.model_type != \"distilbert\":\n",
        "                inputs[\"token_type_ids\"] = batch[2]\n",
        "            outputs = model(**inputs)\n",
        "            _, (intent_logits, slot_logits) = outputs[:2]\n",
        "\n",
        "            # Intent Prediction\n",
        "            if intent_preds is None:\n",
        "                intent_preds = intent_logits.detach().cpu().numpy()\n",
        "            else:\n",
        "                intent_preds = np.append(intent_preds, intent_logits.detach().cpu().numpy(), axis=0)\n",
        "\n",
        "            # Slot prediction\n",
        "            if slot_preds is None:\n",
        "                if args.use_crf:\n",
        "                    # decode() in `torchcrf` returns list with best index directly\n",
        "                    slot_preds = np.array(model.crf.decode(slot_logits))\n",
        "                else:\n",
        "                    slot_preds = slot_logits.detach().cpu().numpy()\n",
        "                all_slot_label_mask = batch[3].detach().cpu().numpy()\n",
        "            else:\n",
        "                if args.use_crf:\n",
        "                    slot_preds = np.append(slot_preds, np.array(model.crf.decode(slot_logits)), axis=0)\n",
        "                else:\n",
        "                    slot_preds = np.append(slot_preds, slot_logits.detach().cpu().numpy(), axis=0)\n",
        "                all_slot_label_mask = np.append(all_slot_label_mask, batch[3].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    intent_preds = np.argmax(intent_preds, axis=1)\n",
        "\n",
        "    if not args.use_crf:\n",
        "        slot_preds = np.argmax(slot_preds, axis=2)\n",
        "\n",
        "    slot_label_map = {i: label for i, label in enumerate(slot_label_lst)}\n",
        "    slot_preds_list = [[] for _ in range(slot_preds.shape[0])]\n",
        "\n",
        "    for i in range(slot_preds.shape[0]):\n",
        "        for j in range(slot_preds.shape[1]):\n",
        "            if all_slot_label_mask[i, j] != pad_token_label_id:\n",
        "                slot_preds_list[i].append(slot_label_map[slot_preds[i][j]])\n",
        "\n",
        "    # Write to output file\n",
        "    with open(pred_config.output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        for words, slot_preds, intent_pred in zip(lines, slot_preds_list, intent_preds):\n",
        "            line = \"\"\n",
        "            for word, pred in zip(words, slot_preds):\n",
        "                if pred == 'O':\n",
        "                    line = line + word + \" \"\n",
        "                else:\n",
        "                    line = line + \"[{}:{}] \".format(word, pred)\n",
        "            f.write(\"<{}> -> {}\\n\".format(intent_label_lst[intent_pred], line.strip()))\n",
        "\n",
        "    logger.info(\"Prediction Done!\")\n",
        "\n",
        "\n",
        "init_logger()\n",
        "parser = argparse.ArgumentParser()\n",
        "\n",
        "parser.add_argument(\"--input_file\", default=\"input.txt\", type=str, help=\"Input file for prediction\")\n",
        "parser.add_argument(\"--output_file\", default=\"output.txt\", type=str, help=\"Output file for prediction\")\n",
        "parser.add_argument(\"--model_dir\", default=\"atis_model\", type=str, help=\"Path to save, load model\")\n",
        "\n",
        "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size for prediction\")\n",
        "parser.add_argument(\"--no_cuda\", action=\"store_true\", help=\"Avoid using CUDA when available\")\n",
        "\n",
        "pred_config = parser.parse_args([])\n",
        "predict(pred_config)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}